#!/usr/bin/env python3
"""
Diagn√≥stico do tokenizer SentencePiece v4k_bf.
Investiga problema de byte-fallback na decodifica√ß√£o.
"""

import sentencepiece as spm
from pathlib import Path

# Carregar tokenizer
tokenizer_path = Path("versions/v4-subword-lstm/tokenizer_v4k_bf/spm_v4k_bf.model")
sp = spm.SentencePieceProcessor()
sp.Load(str(tokenizer_path))

print("=" * 80)
print("DIAGN√ìSTICO: Tokenizer SentencePiece v4k_bf")
print("=" * 80)

# Informa√ß√µes b√°sicas
print(f"\nVocab size: {sp.vocab_size()}")
print(f"BOS ID: {sp.bos_id()}")
print(f"EOS ID: {sp.eos_id()}")
print(f"UNK ID: {sp.unk_id()}")
print(f"PAD ID: {sp.pad_id()}")

# Teste com texto limpo
test_text = "O setor de infraestrutura log√≠stica brasileira debate novas concess√µes"
print(f"\n{'‚îÄ' * 80}")
print("TESTE 1: Texto limpo")
print(f"Input: {test_text}")

# Encode
ids = sp.EncodeAsIds(test_text)
pieces = sp.EncodeAsPieces(test_text)

print(f"\nIDs ({len(ids)}): {ids[:20]}...")
print(f"Pieces ({len(pieces)}): {pieces[:20]}...")

# Decode
decoded = sp.DecodeIds(ids)
print(f"\nDecoded: {decoded}")
print(f"Match original: {decoded == test_text.lower()}")

# Verificar tokens espec√≠ficos problem√°ticos dos samples
print(f"\n{'‚îÄ' * 80}")
print("TESTE 2: Tokens problem√°ticos dos samples")

# Pegar alguns tokens que aparecem nos samples ruins
problematic_tokens = [
    "cl√≠n",
    "encamatos",
    "suas√≥veis√µe",
    "clubeson",
    "plapenho",
    "penÔøΩ",
]

print("\nTokens problem√°ticos encontrados nos samples:")
for token in problematic_tokens:
    ids = sp.EncodeAsIds(token)
    pieces = sp.EncodeAsPieces(token)
    decoded = sp.DecodeIds(ids)
    print(f"  '{token}' -> IDs:{ids} -> Pieces:{pieces} -> Decoded:'{decoded}'")

# Verificar byte-fallback tokens
print(f"\n{'‚îÄ' * 80}")
print("TESTE 3: Byte-fallback tokens no vocabul√°rio")

byte_fallback_count = 0
byte_fallback_examples = []

for i in range(min(sp.vocab_size(), 4000)):
    piece = sp.IdToPiece(i)
    if piece.startswith('<0x'):  # Byte-fallback token
        byte_fallback_count += 1
        if len(byte_fallback_examples) < 10:
            byte_fallback_examples.append((i, piece))

print(f"\nTotal byte-fallback tokens: {byte_fallback_count}")
print(f"Exemplos: {byte_fallback_examples}")

# Testar decodifica√ß√£o de sequ√™ncia com byte-fallback
print(f"\n{'‚îÄ' * 80}")
print("TESTE 4: Decodificar sample real do modelo")

# Pegar primeiros tokens do sample problem√°tico
sample_start = "cl√≠n informa√ß√µes viagem claro"
ids_sample = sp.EncodeAsIds(sample_start)
pieces_sample = sp.EncodeAsPieces(sample_start)

print(f"\nSample start: {sample_start}")
print(f"IDs: {ids_sample}")
print(f"Pieces: {pieces_sample}")
print(f"Decoded: {sp.DecodeIds(ids_sample)}")

# Verificar se "cl√≠n" √© uma palavra v√°lida ou fragmento
print(f"\n{'‚îÄ' * 80}")
print("TESTE 5: An√°lise de 'cl√≠n' (primeiro token problem√°tico)")

clin_ids = sp.EncodeAsIds("cl√≠n")
clin_pieces = sp.EncodeAsPieces("cl√≠n")
print(f"'cl√≠n' -> IDs: {clin_ids}, Pieces: {clin_pieces}")

# Tentar varia√ß√µes
for variant in ["cl√≠nica", "cl√≠nico", "cl√≠n", "clin"]:
    ids = sp.EncodeAsIds(variant)
    pieces = sp.EncodeAsPieces(variant)
    decoded = sp.DecodeIds(ids)
    print(f"  '{variant}' -> {len(ids)} tokens -> Pieces:{pieces} -> '{decoded}'")

# Testar texto portugu√™s v√°lido
print(f"\n{'‚îÄ' * 80}")
print("TESTE 6: Texto portugu√™s v√°lido longo")

valid_text = """
A infraestrutura log√≠stica brasileira enfrenta desafios importantes.
O setor debate novas concess√µes ferrovi√°rias e metas de produtividade.
As empresas buscam integra√ß√£o com portos para melhorar os corredores de exporta√ß√£o.
"""

ids_valid = sp.EncodeAsIds(valid_text.strip())
decoded_valid = sp.DecodeIds(ids_valid)

print(f"Texto original ({len(valid_text.strip())} chars):")
print(valid_text.strip())
print(f"\nTokens: {len(ids_valid)}")
print(f"\nDecoded ({len(decoded_valid)} chars):")
print(decoded_valid)
print(f"\nMatch: {decoded_valid == valid_text.strip().lower()}")

# Verificar caracteres especiais no decoded
special_chars = [c for c in decoded_valid if ord(c) > 127 or c == 'ÔøΩ']
if special_chars:
    print(f"\nCaracteres especiais encontrados: {set(special_chars)}")
    print(f"Contagem de 'ÔøΩ': {decoded_valid.count('ÔøΩ')}")

print("\n" + "=" * 80)
print("CONCLUS√ÉO:")
print("=" * 80)

# An√°lise final
if decoded_valid == valid_text.strip().lower():
    print("‚úÖ Tokenizer decodifica corretamente texto portugu√™s v√°lido")
else:
    print("‚ùå Tokenizer tem problemas com decodifica√ß√£o de texto portugu√™s")

if byte_fallback_count > 0:
    print(f"‚ö†Ô∏è  Vocabul√°rio cont√©m {byte_fallback_count} tokens de byte-fallback")
    print("   ‚Üí Isso pode causar fragmenta√ß√£o em palavras n√£o vistas no treino")

print("\nüí° HIP√ìTESE:")
print("   Os samples ruins podem ser causados por:")
print("   1. Modelo gerando sequ√™ncias de tokens inv√°lidas")
print("   2. Temperatura/sampling gerando tokens raros de byte-fallback")
print("   3. Modelo n√£o aprendeu padr√µes corretos de composi√ß√£o de subwords")
print("=" * 80)
