{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Setup\n",
    "\n",
    "Define os caminhos padronizados para salvar os artefatos do modelo e dos mapeamentos da versão v2. \n",
    "Cria os diretórios necessários para armazenar os arquivos de modelo treinado (`modelo_char_lm_v2.keras`) e mapeamentos de caracteres (`mapeamentos_v2.pkl`).\n",
    "\n",
    "\n",
    "- MODELO_OUT → versions/v2-char-lm/models/modelo_char_lm_v2.keras\n",
    "- MAPEAMENTOS_OUT → versions/v2-char-lm/mappings/mapeamentos_v2.pkl\n",
    "\n",
    "Esses caminhos organizam os arquivos de treino e facilitam reuso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: D:\\Dropbox\\Coding\\TCC\\versions\\v2-char-lm\\models\\modelo_char_lm_v2.keras\n",
      "Mapeamentos: D:\\Dropbox\\Coding\\TCC\\versions\\v2-char-lm\\mappings\\mapeamentos_v2.pkl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd().resolve()\n",
    "if BASE_DIR.name == 'notebooks':\n",
    "    BASE_DIR = BASE_DIR.parent\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "MAPPINGS_DIR = BASE_DIR / 'mappings'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MAPPINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELO_OUT = MODELS_DIR / 'modelo_char_lm_v2.keras'\n",
    "MAPEAMENTOS_OUT = MAPPINGS_DIR / 'mapeamentos_v2.pkl'\n",
    "print('Modelo:', MODELO_OUT)\n",
    "print('Mapeamentos:', MAPEAMENTOS_OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dados e Pré-Processamento\n",
    "\n",
    "Esta etapa faz o download do texto completo do livro \"Dom Casmurro\" diretamente do Projeto Gutenberg, utilizando a função `tf.keras.utils.get_file` para garantir que o arquivo seja salvo localmente. Em seguida, o texto é carregado e decodificado para UTF-8, ignorando possíveis erros de codificação. O texto é normalizado convertendo todos os caracteres para minúsculo (`lower()`) e comprimindo múltiplos espaços em apenas um, o que reduz ruídos e inconsistências comuns em textos literários. Por fim, o tamanho total do corpus em caracteres é exibido, permitindo avaliar a quantidade de dados disponível para o treinamento do modelo de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do corpus (chars): 376658\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf, os, pickle\n",
    "\n",
    "URL_LIVRO = 'https://www.gutenberg.org/files/55752/55752-0.txt'\n",
    "caminho_arquivo = tf.keras.utils.get_file('dom_casmurro.txt', URL_LIVRO)\n",
    "with open(caminho_arquivo, 'rb') as f:\n",
    "    texto = f.read().decode('utf-8', errors='ignore')\n",
    "texto = texto.lower()\n",
    "texto = ' '.join(texto.split())\n",
    "print('Tamanho do corpus (chars):', len(texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Vocabulário e Janelas de Treinamento\n",
    "\n",
    "Nesta etapa, extraímos todos os caracteres únicos presentes no corpus para formar o vocabulário do modelo. Em seguida, criamos dois mapeamentos essenciais: \n",
    "- `char_to_id`: converte cada caractere em um índice inteiro único.\n",
    "- `id_to_char`: faz o caminho inverso, permitindo reconstruir texto a partir de índices.\n",
    "\n",
    "Esses mapeamentos são salvos em disco (`MAPEAMENTOS_OUT`) para garantir reprodutibilidade e facilitar o uso futuro sem necessidade de recalcular.\n",
    "\n",
    "Definimos o comprimento da sequência de entrada (`SEQ_LEN=160`), que determina quantos caracteres o modelo observa antes de prever o próximo. Para alimentar o modelo, implementamos um gerador de batches que, sob demanda, produz pares de janelas (one-hot) e seus respectivos rótulos (próximo caractere), otimizando memória e desempenho durante o treinamento.\n",
    "\n",
    "Por fim, calculamos o número aproximado de steps por época, considerando o tamanho do corpus e o batch size, o que auxilia no planejamento do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: 66 caracteres\n",
      "steps_per_epoch (aprox): 1470\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "SEQ_LEN = 160\n",
    "caracteres = sorted(list(set(texto)))\n",
    "char_to_id = {c: i for i, c in enumerate(caracteres)}\n",
    "id_to_char = {i: c for i, c in enumerate(caracteres)}\n",
    "vocab = len(caracteres)\n",
    "with open(MAPEAMENTOS_OUT, 'wb') as f:\n",
    "    pickle.dump({'char_to_id': char_to_id, 'id_to_char': id_to_char, 'tamanho_sequencia': SEQ_LEN}, f)\n",
    "print('Vocabulario:', vocab, 'caracteres')\n",
    "\n",
    "def batch_generator(texto, char2idx, seq_len=160, batch_size=256, step=1):\n",
    "    n = len(texto) - seq_len\n",
    "    V = len(char2idx)\n",
    "    i = 0\n",
    "    while True:\n",
    "        X = np.zeros((batch_size, seq_len, V), dtype=np.float32)\n",
    "        y = np.zeros((batch_size,), dtype=np.int32)\n",
    "        for b in range(batch_size):\n",
    "            idx = (i + b*step) % n\n",
    "            seq = texto[idx: idx+seq_len]\n",
    "            nxt = texto[idx+seq_len]\n",
    "            for t, ch in enumerate(seq):\n",
    "                X[b, t, char2idx.get(ch, 0)] = 1.0\n",
    "            y[b] = char2idx.get(nxt, 0)\n",
    "        i = (i + batch_size*step) % n\n",
    "        yield X, y\n",
    "\n",
    "steps_per_epoch = max(1, (len(texto) - SEQ_LEN) // 256)\n",
    "print('steps_per_epoch (aprox):', steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Arquitetura do Modelo\n",
    "\n",
    "Nesta etapa, definimos a arquitetura da rede neural responsável por aprender padrões de sequência de caracteres do texto. Utilizamos uma camada LSTM com 512 unidades, que é especialmente adequada para capturar dependências de longo prazo em dados sequenciais, como texto. A entrada do modelo é uma sequência de vetores one-hot, cada um representando um caractere do vocabulário, com dimensão (`SEQ_LEN`, `vocab`). Após processar a sequência, a LSTM gera uma representação interna que é passada para uma camada densa (`Dense`) com ativação softmax, responsável por prever a probabilidade de cada caractere possível como próximo elemento da sequência.\n",
    "\n",
    "O modelo é compilado com o otimizador Adam, que acelera e estabiliza o treinamento, utilizando uma taxa de aprendizado inicial de 0.002 e normalização de gradiente (`clipnorm=1.0`) para evitar explosão de gradientes. A função de perda escolhida é `sparse_categorical_crossentropy`, adequada para classificação multi-classe com rótulos inteiros, e a métrica de avaliação é acurácia.\n",
    "\n",
    "Essa configuração permite ao modelo aprender a prever o próximo caractere em uma sequência, baseando-se no contexto dos caracteres anteriores, e serve como base para geração de texto e outras tarefas de modelagem de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.LSTM(512, input_shape=(SEQ_LEN, vocab)),\n",
    "    layers.Dense(vocab, activation='softmax'),\n",
    "])\n",
    "optimizer = optimizers.Adam(learning_rate=2e-3, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Treinamento\n",
    "\n",
    "Nesta etapa, o modelo é treinado para aprender padrões de sequência de caracteres do corpus. Utilizamos um gerador de batches (`batch_generator`) que alimenta o modelo com janelas de texto e seus respectivos rótulos, otimizando o uso de memória ao processar grandes volumes de dados.\n",
    "\n",
    "O treinamento é configurado com os seguintes parâmetros:\n",
    "- `BATCH_SIZE=256`: número de sequências processadas por batch.\n",
    "- `EPOCHS=40`: número máximo de épocas de treinamento.\n",
    "\n",
    "Três callbacks são utilizados para tornar o processo mais eficiente e robusto:\n",
    "- `ModelCheckpoint`: salva automaticamente o melhor modelo encontrado durante o treinamento, monitorando a função de perda (`loss`).\n",
    "- `ReduceLROnPlateau`: reduz a taxa de aprendizado quando a perda para de melhorar, facilitando a convergência.\n",
    "- `EarlyStopping`: interrompe o treinamento caso não haja melhora na perda por várias épocas, evitando overfitting.\n",
    "\n",
    "O método `fit` executa o treinamento usando o gerador, com o número de steps por época calculado a partir do tamanho do corpus e do batch size. Ao final, o melhor modelo é salvo no caminho especificado, pronto para ser utilizado na geração de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks as kb\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 40\n",
    "monitor='loss'\n",
    "cb = [\n",
    "    kb.ModelCheckpoint(str(MODELO_OUT), save_best_only=True, monitor=monitor, mode='min'),\n",
    "    kb.ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=3, min_lr=5e-5),\n",
    "    kb.EarlyStopping(monitor=monitor, patience=5, restore_best_weights=True),\n",
    "]\n",
    "gen = batch_generator(texto, char_to_id, seq_len=SEQ_LEN, batch_size=BATCH_SIZE, step=1)\n",
    "steps_per_epoch = max(1, (len(texto) - SEQ_LEN) // BATCH_SIZE)\n",
    "history = model.fit(gen, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, callbacks=cb, verbose=1)\n",
    "print('Treinamento concluido! Melhor modelo salvo em:', MODELO_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Salvamento e Carregamento\n",
    "\n",
    "Nesta etapa, garantimos que os artefatos do modelo treinado e dos mapeamentos de caracteres estejam prontos para uso em geração de texto, sem necessidade de retreinar o modelo.\n",
    "\n",
    "Primeiro, o modelo é carregado do disco usando o caminho definido em `MODELO_OUT` (caso o arquivo exista). Se o arquivo não estiver presente, utiliza-se o modelo já instanciado e treinado na sessão atual. Isso permite flexibilidade para continuar o trabalho mesmo se o notebook for reiniciado ou transferido para outro ambiente.\n",
    "Em seguida, os mapeamentos de caracteres (`char_to_id` e `id_to_char`) são recarregados do arquivo `MAPEAMENTOS_OUT` usando pickle. Esses mapeamentos são essenciais para converter texto em índices inteiros (para alimentar o modelo) e para reconstruir texto a partir das previsões do modelo.\n",
    "\n",
    "Por fim, as variáveis `modelo`, `c2i` (char_to_id) e `i2c` (id_to_char) ficam disponíveis para geração de texto, garantindo que todo o pipeline de inferência esteja pronto e reprodutível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artefatos prontos para gerar texto.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf, pickle\n",
    "modelo = tf.keras.models.load_model(MODELO_OUT) if MODELO_OUT.exists() else model\n",
    "with open(MAPEAMENTOS_OUT, 'rb') as f:\n",
    "    maps = pickle.load(f)\n",
    "c2i = maps['char_to_id']\n",
    "i2c = maps['id_to_char']\n",
    "print('Artefatos prontos para gerar texto.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Geração de Texto\n",
    "\n",
    "Nesta etapa, implementamos a geração automática de texto usando o modelo treinado de linguagem de caracteres. O processo utiliza uma função de amostragem top-k, que limita as escolhas do próximo caractere aos k mais prováveis, tornando a geração mais criativa e menos previsível. A temperatura controla o grau de aleatoriedade: valores baixos tornam as escolhas mais conservadoras, enquanto valores altos aumentam a diversidade.\n",
    "\n",
    "O procedimento começa com uma semente (`seed`), normalmente um trecho do corpus com cerca de 160 caracteres, que serve como contexto inicial para o modelo. A cada passo, o modelo recebe a sequência atual (codificada em one-hot) e prevê a distribuição de probabilidade para o próximo caractere. A função `sample_top_k` seleciona o próximo caractere entre os k mais prováveis, ponderando pela temperatura.\n",
    "\n",
    "Esse ciclo se repete até atingir o comprimento desejado (`comprimento=400`), gerando texto novo que segue o estilo e padrões aprendidos do corpus original. O resultado é uma sequência textual que pode ser usada para análise, criatividade ou avaliação do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eram viver, e o ceu estava ainda no caixão e escobar e o sol claro, cuidar do cerro do dia. seguir-me alvoroçado e rir, e escobar comia uma semiração acceitei o seu perante; mas dimo-lhe o que me diseu e quando le ouvir-lhe o meu nosso suspero; de santiffeira padre-nossos e como o diabo. ezequiel apertar a minha esposa ao fim do pedaco, e vamos. este fiquei o mais. isto é, disse que, beijava-me a \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sample_top_k(probs, k=20, temperature=0.8, rng=None):\n",
    "    probs = np.asarray(probs, dtype=np.float64)\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    k = int(max(1, min(k, probs.size)))\n",
    "    top_idx = np.argpartition(-probs, k-1)[:k]\n",
    "    sel = probs[top_idx]\n",
    "    if temperature > 0:\n",
    "        logits = np.log(np.maximum(sel, 1e-9)) / temperature\n",
    "        logits -= logits.max()\n",
    "        sel = np.exp(logits)\n",
    "    p = sel / sel.sum()\n",
    "    return int(top_idx[rng.choice(len(top_idx), p=p)])\n",
    "\n",
    "def gerar_texto(model, char_to_id, id_to_char, seed, comprimento=400, k=20, temperatura=0.8):\n",
    "    vocab = len(id_to_char)\n",
    "    seq_len = model.input_shape[1] if isinstance(model.input_shape, (list,tuple)) else 160\n",
    "    rep = ' ' if ' ' in char_to_id else next(iter(char_to_id.keys()))\n",
    "    seed = ''.join(ch if ch in char_to_id else rep for ch in seed)\n",
    "    ids = [char_to_id[ch] for ch in seed][-seq_len:]\n",
    "    if len(ids) < seq_len:\n",
    "        pad = char_to_id.get(' ', ids[0] if ids else 0)\n",
    "        ids = [pad] * (seq_len - len(ids)) + ids\n",
    "    out = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    for _ in range(comprimento):\n",
    "        X = np.zeros((1, seq_len, vocab), dtype=np.float32)\n",
    "        for t, idx in enumerate(ids):\n",
    "            if idx < vocab: X[0, t, idx] = 1.0\n",
    "        p = model.predict(X, verbose=0)[0]\n",
    "        nxt = sample_top_k(p, k=k, temperature=temperatura, rng=rng)\n",
    "        out.append(id_to_char.get(nxt, '?'))\n",
    "        ids = ids[1:] + [nxt]\n",
    "    return ''.join(out)\n",
    "\n",
    "seed = 'os melhores meios de executar o trabalho '\n",
    "print(gerar_texto(modelo, c2i, i2c, seed, comprimento=400, k=20, temperatura=0.8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
