{
  "tokenization": "sentencepiece",
  "tokenizer_model": "/root/TCC/versions/v4-subword-lstm/tokenizer_v4k_bf/spm_v4k_bf.model",
  "tokenizer_vocab_size": 4000,
  "sequence_length": 320,
  "stride": 2,
  "batch_size": 192,
  "add_bos": true,
  "add_eos": true,
  "model": {
    "d_model": 384,
    "num_layers": 6,
    "num_heads": 6,
    "d_ff": 1536,
    "dropout": 0.1
  }
}