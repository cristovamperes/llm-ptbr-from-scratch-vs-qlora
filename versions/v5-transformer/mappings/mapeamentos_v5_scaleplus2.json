{
  "tokenization": "sentencepiece",
  "tokenizer_model": "/workspace/TCC/versions/v4-subword-lstm/tokenizer_v4k_bf/spm_v4k_bf.model",
  "tokenizer_vocab_size": 4000,
  "sequence_length": 384,
  "stride": 32,
  "batch_size": 18,
  "add_bos": true,
  "add_eos": true,
  "model": {
    "d_model": 512,
    "num_layers": 6,
    "num_heads": 8,
    "d_ff": 2048,
    "dropout": 0.15
  }
}